{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7158699d-253d-4134-a370-ccf54ea7d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d42bfe-86f4-4f41-8c88-74fd056cdef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, compressive_memory_size, segment_length):\n",
    "        super(InfiniAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.segment_length = segment_length\n",
    "        \n",
    "        # Ensure d_model is divisible by n_heads for multi-head attention\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Projection matrices for Query, Key, Value\n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, d_model)\n",
    "        self.WV = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Compressive memory matrix\n",
    "        self.memory = torch.zeros((compressive_memory_size, self.d_k))\n",
    "        self.memory_key_sum = torch.zeros((self.d_k,))\n",
    "        \n",
    "        # Output projection\n",
    "        self.WO = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Gating parameter for combining local and memory states\n",
    "        self.beta = nn.Parameter(torch.tensor(0.0))\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn_weights, V), attn_weights\n",
    "\n",
    "    def compressive_memory_retrieve(self, Q):\n",
    "        # Reshape Q to match the dimensions for matrix multiplication\n",
    "        Q = Q.view(Q.size(0), Q.size(1), self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_length, d_k)\n",
    "        \n",
    "        retrieval = torch.matmul(F.elu(Q) + 1, self.memory.T)  # (batch_size, n_heads, seq_length, compressive_memory_size)\n",
    "        normalization = torch.sum(F.elu(Q) + 1, dim=-1, keepdim=True)\n",
    "        return retrieval / (normalization + 1e-6)\n",
    "    \n",
    "    def compressive_memory_update(self, K, V):\n",
    "        # Reshape K and V to match the dimensions for matrix multiplication\n",
    "        K = K.view(K.size(0), K.size(1), self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_length, d_k)\n",
    "        V = V.view(V.size(0), V.size(1), self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_length, d_k)\n",
    "        \n",
    "        self.memory += torch.matmul(F.elu(K).transpose(-2, -1) + 1, V)\n",
    "        self.memory_key_sum += torch.sum(F.elu(K) + 1, dim=0)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Compute Q, K, V\n",
    "        Q = self.WQ(X)\n",
    "        K = self.WK(X)\n",
    "        V = self.WV(X)\n",
    "        \n",
    "        # Local scaled dot-product attention\n",
    "        local_context, _ = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Compressive memory retrieval\n",
    "        global_context = self.compressive_memory_retrieve(Q)\n",
    "        \n",
    "        # Combine local and global contexts (Equation 10)\n",
    "        combined_context = torch.sigmoid(self.beta) * global_context + (1 - torch.sigmoid(self.beta)) * local_context\n",
    "        \n",
    "        # Update memory with the current segment's K and V\n",
    "        self.compressive_memory_update(K, V)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.WO(combined_context.transpose(1, 2).contiguous().view(X.size(0), -1, self.d_model))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e213d8-3ed0-47cb-b23c-77f642ec385d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Dummy input: Batch size of 10, segment length of 2048, model dimension of 512\u001b[39;00m\n\u001b[1;32m     10\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, segment_length, d_model)\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43minfi_attention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected shape: [10, 2048, 512]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/Robotics/VLA/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/Robotics/VLA/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m, in \u001b[0;36mInfiniAttention.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     59\u001b[0m global_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompressive_memory_retrieve(Q)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Combine local and global contexts (Equation 10)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m combined_context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mglobal_context\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlocal_context\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Update memory with the current segment's K and V\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompressive_memory_update(K, V)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "compressive_memory_size = 1024\n",
    "segment_length = 2048\n",
    "\n",
    "infi_attention_layer = InfiniAttention(d_model, n_heads, compressive_memory_size, segment_length)\n",
    "\n",
    "# Dummy input: Batch size of 10, segment length of 2048, model dimension of 512\n",
    "input_data = torch.randn(10, segment_length, d_model)\n",
    "output = infi_attention_layer(input_data)\n",
    "print(output.shape)  # Expected shape: [10, 2048, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fa4e3-64ab-40e8-aeb7-bd148b4ef5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
